# -*- coding: utf-8 -*-
"""water potability

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GPxXln-kuD9ouJoMgJQam6LLAZrApAhM

## LOGISTIC REGRESSION
"""

import pandas as pd
df = pd.read_csv('water_potability.csv')
print(df.head())

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
RANDOM_STATE= 42

#check where and in which column there is null values using count()
#Take mean of that column and substitute mean value at the null value.
null_values = df.isnull()
null_counts = null_values.sum(axis=0)
columns_with_null = null_counts[null_counts > 0].index.tolist()
print("Columns with null values:", columns_with_null)

df.mean()

import pandas as pd
mean_values = df.mean()

# Fill missing values with mean values for each column
df.fillna(mean_values, inplace=True)

# Display the DataFrame to verify the changes
print(df)

y= df['Potability']
print(y)

x=df.drop('Potability',axis=1)
print(x)

from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler()
x1 = sc.fit_transform(x)
print(x1)

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x1,y,test_size=0.20,random_state=20)

print(x_train.shape)

from sklearn.linear_model import LogisticRegression
s = LogisticRegression()
model=s.fit(x_train,y_train)

yp = model.predict(x_test)

from sklearn.metrics import accuracy_score
print(accuracy_score(y_test,yp))

from sklearn.metrics import classification_report,confusion_matrix
print(classification_report(y_test,yp))

from imblearn.over_sampling import SMOTE

# Assuming 'inputs_train' and 'target_train' are your training data and labels
smote = SMOTE(random_state=42)
x_train_smote, y_train_smote = smote.fit_resample(x_train, y_train)

# Now you can check the value counts to confirm balancing
print(y_train_smote.value_counts())

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'model' is your trained model and 'inputs_test' is your test data
predictions = model.predict(x_test)

# Now you can create the confusion matrix with the true labels and your predictions
confusion_mat = confusion_matrix(y_test, predictions)

# Visualize the confusion matrix
sns.heatmap(confusion_mat, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""# KNN"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
RANDOM_STATE = 42

from sklearn.model_selection import train_test_split
x=df.drop('Potability',axis=1)
y= df['Potability']
x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=RANDOM_STATE)
print(y_train.value_counts())
print(y_test.value_counts())

y_train = y_train.astype('int')
y_test = y_test.astype('int')

from sklearn.neighbors import KNeighborsClassifier

def knn_model_fitting(x_train, x_test, y_train, y_test, n_neighbors):

    knn = KNeighborsClassifier(n_neighbors = n_neighbors)
    knn.fit(x_train, y_train)

    preds = knn.predict(x_test)
    score = knn.score(x_test, y_test)
    print('The mean accuracy of this KNN classifier is: {}'.format(score))

    return preds, knn
# here just set the # of neighbors =1 to see the model performance
n_neighbors= 1
preds_default, knn_default = knn_model_fitting(x_train, x_test, y_train, y_test, n_neighbors)

import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Define a list to store the accuracies for different K values
accuracies = []

# Loop through different K values from 2 to 20
for k in range(150, 201):
    # Initialize and fit the KNN classifier with the current K value
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(x_train, y_train)

    # Predict labels for the test data
    y_pred = knn.predict(x_test)

    # Calculate the accuracy for the current K value
    accuracy = accuracy_score(y_test, y_pred)

    # Append the accuracy to the list
    accuracies.append(accuracy)

# Plot the accuracy vs K graph
plt.figure(figsize=(10, 6))
plt.plot(range(150, 201), accuracies, color="blue", linestyle="--", marker="o", markersize=10)

# Set labels and title
plt.xlabel("K Value")
plt.ylabel("Accuracy")
plt.title("Accuracy vs K Value for KNN")

# Display the plot
plt.show()

"""# DECISION TREE"""

df=pd.read_csv("water_potability.csv")
df

df.fillna(0,inplace=True)

inputs =df.drop('Potability',axis='columns')
target =df['Potability']

inputs

target

from sklearn import tree
model = tree.DecisionTreeClassifier()

model.fit(inputs,target)

model.score(inputs,target)

model.predict([[8.316766,	214.373394,	22018.41744,8.059332,356.886136,363.266516,18.436525,100.341674,4.628771]])

from sklearn.model_selection import train_test_split
inputs_train,inputs_test,target_train,target_test=train_test_split(inputs,target,random_state=RANDOM_STATE)
print(target_train.value_counts())
print(target_test.value_counts())

target_train=target_train.astype('int')
target_test=target_test.astype('int')

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

target_pred = model.predict(inputs_test)

accuracy = accuracy_score(target_test, target_pred)
precision = precision_score(target_test, target_pred)
recall = recall_score(target_test, target_pred)
f1 = f1_score(target_test, target_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-Score:", f1)

"""# HEAT MAP"""

from sklearn.model_selection import train_test_split

# Replace 'inputs' and 'target' with your actual data and labels
inputs_train, inputs_test, target_train, target_test = train_test_split(inputs, target, random_state=42)

from imblearn.over_sampling import SMOTE

# Assuming 'inputs_train' and 'target_train' are your training data and labels
smote = SMOTE(random_state=42)
inputs_train_smote, target_train_smote = smote.fit_resample(inputs_train, target_train)

# Now you can check the value counts to confirm balancing
print(target_train_smote.value_counts())

from sklearn.metrics import confusion_matrix

target_pred = model.predict(inputs_test)

cm = confusion_matrix(target_test, target_pred)
sns.heatmap(cm, annot=True)
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'model' is your trained model and 'inputs_test' is your test data
predictions = model.predict(inputs_test)

# Now you can create the confusion matrix with the true labels and your predictions
confusion_mat = confusion_matrix(target_test, predictions)

# Visualize the confusion matrix
sns.heatmap(confusion_mat, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""SUPPORT VECTOR MACHINE"""

import pandas as pd

dir(df)

df.head()

df.Potability

x=df.iloc[:,0:9]

y=df.iloc[:,9]
y

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)
len(x_train)

len(x_test)

from sklearn.svm import SVC
model=SVC(kernel='linear')
model.fit(x_train,y_train)

model.predict(x_test)

x_test

model.score(x_test,y_test)

from sklearn.svm import SVC
model=SVC(kernel='rbf')
model.fit(x_train,y_train)

model.predict(x_test)

x_test

model.score(x_test,y_test)

from sklearn.svm import SVC
model=SVC(kernel='poly')
model.fit(x_train,y_train)

model.score(x_test,y_test)

import matplotlib.pyplot as plt
import seaborn as sns

# Create a heatmap of the correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(x_train.corr(), annot=True, cmap="YlGnBu")

# Set axis labels and title
plt.xlabel("Features")
plt.ylabel("Features")
plt.title("Correlation Heatmap of Training Features")

# Show the plot
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Initialize the classifier
rfc = RandomForestClassifier(n_estimators=100, random_state=42)

# Fit the model with training data
rfc.fit(x_train, y_train)

# Predict classes for new data
predictions = rfc.predict(x_test)

# Evaluate the model performance
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:",accuracy)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(y_test, predictions)
precision = precision_score(y_test, predictions)
recall = recall_score(y_test,predictions)
f1 = f1_score(y_test, predictions)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-Score:", f1)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score


water_df = pd.read_csv('water_potability.csv')

# Handle missing values (NaN)
imputer = SimpleImputer(strategy='mean')
X = imputer.fit_transform(water_df.drop(columns=['Potability']))
y = water_df['Potability']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)
gbm.fit(X_train, y_train)

y_pred = gbm.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"GBM Accuracy: {accuracy:.4f}")

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the dataset
#df = pd.read_csv('water_potability.csv')

# Assuming the target variable is named 'Potability' and all other columns are features
X = df.drop('Potability', axis=1)
y = df['Potability']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the XGBoost classifier
xgb_classifier = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)
xgb_classifier.fit(X_train, y_train)

# Make predictions and calculate accuracy
y_pred = xgb_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

# Print the accuracy
print(f"XGBoost Accuracy: {accuracy:.4f}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the water potability dataset
#df = pd.read_csv('water_potability.csv')  # Make sure to provide the correct path to the file

# Separate features and target variable
X = df.drop('Potability', axis=1)
y = df['Potability']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Gradient Boosting Classifier
gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)

# Fit the model to the training data
gbm.fit(X_train, y_train)

# Make predictions on the test data
y_pred = gbm.predict(X_test)

# Calculate the performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1score = f1_score(y_test, y_pred)

# Print the performance metrics
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1score:.4f}")